{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder-Decoder.ipynb",
      "provenance": [],
      "mount_file_id": "1YVEQjl7UBE7ySYRNACqARL9DM4uNDnoT",
      "authorship_tag": "ABX9TyMOSsYQBTO/s7yLA81l3JTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wapaa/Projects/blob/master/Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5H3TWWzQgMaa"
      },
      "outputs": [],
      "source": [
        "#Importing library\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons spécifier la valeur des hyperparamètres, y compris la taille du batch pour l'apprentissage, la dimension latente pour l'espace d'encodage et le nombre d'échantillons sur lesquels s'entraîner."
      ],
      "metadata": {
        "id": "QbPIBwfQgpjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 256\n",
        "num_samples = 10000"
      ],
      "metadata": {
        "id": "UDB3g86-ghCK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La modélisation séquence à séquence (Seq2Seq) consiste à entraîner les modèles capables de convertir des séquences d'un domaine en séquences d'un autre domaine, par exemple, de l'anglais au français. Cette modélisation Seq2Seq est réalisée par l'encodeur et le décodeur\n",
        "\n",
        "Nous mettrons en œuvre l'apprentissage en profondeur dans le modèle Seq2Seq (Seq2Seq) pour la traduction des langues. Cette approche sera appliquée pour convertir les phrases courtes en anglais à des phrases françaises correspondantes. L'encodeur et le décodeur LSTM sont utilisés pour traiter le modèle séquence à séquence dans cette tâche.\n",
        "\n",
        "\n",
        "Le dataset est récupéré de Kaggle qui est publiquement disponible sous forme de paires bilingues français-anglais. Ce dataset contient des paires de phrases en français ainsi que leur traduction en anglais."
      ],
      "metadata": {
        "id": "QaFaSq55g1Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "\n",
        "with open('/content/drive/MyDrive/archive (3)/fra-eng/fra.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "num_encoder_tokens = len(input_chars)\n",
        "num_decoder_tokens = len(target_chars)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#Print size\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNiukfzQgwXw",
        "outputId": "d892935b-7792-4205-ead7-f7758dee70fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 93\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les lignes de codes ci-dessous effectueront la vectorisation des données où nous lirons le fichier contenant les phrases anglaises et françaises correspondantes. Dans le processus de vectorisation, la collection de documents texte est convertie en vecteurs de caractéristiques."
      ],
      "metadata": {
        "id": "ZOyFfeekhl-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define data for encoder and decoder\n",
        "input_token_id = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_id = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "encoder_in_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_in_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_in_data[i, t, input_token_id[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_in_data[i, t, target_token_id[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_id[char]] = 1."
      ],
      "metadata": {
        "id": "ex8NbPE6hn-F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir obtenu l'ensemble de données avec toutes les features, nous définissons les données d'entrée pour l'enoder et le decoder et ainsi que les données targets pour le décodeur."
      ],
      "metadata": {
        "id": "RSC6gHZ6h4vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and process the input sequence\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "#We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#Using `encoder_states` set up the decoder as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "0_DlZvsGhqTJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "RnhlAyRXh8p_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo1I4FIJh-yl",
        "outputId": "a3ee0bc4-8bdd-4e18-a005-96214324c0dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 71)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 93)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        335872      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  358400      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 93)     23901       ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 718,173\n",
            "Trainable params: 718,173\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling and training the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_in_data, decoder_in_data], decoder_target_data, batch_size = batch_size, epochs=150, validation_split=0.2, verbose=0)"
      ],
      "metadata": {
        "id": "Zf1po3lniASo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "BPwRY4C7iEwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_id.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_id.items())\n",
        "\n",
        "#Define Decode Sequence\n",
        "def decode_sequence(input_seq):\n",
        "    #Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    #Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    #Get the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_id['\\t']] = 1.\n",
        "\n",
        "    #Sampling loop for a batch of sequences\n",
        "    #(to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        #Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        #Exit condition: either hit max length\n",
        "        #or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        #Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        #Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "p4D0xRYUiJD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(10):\n",
        "    input_seq = encoder_in_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "id": "-HM25TMWiPUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ciOX1JACkrt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}